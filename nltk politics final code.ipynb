{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/angelanazarian/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import the packages\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "nltk.download(\"stopwords\") #Import stopwords and punctuation from NLTK\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and form corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump_corpus = nltk.corpus.PlaintextCorpusReader('/Users/angelanazarian/NLTK_Final_Project/Clinton-Trump-Corpus/Trump/','Trump_.*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clinton_corpus = nltk.corpus.PlaintextCorpusReader('/Users/angelanazarian/NLTK_Final_Project/Clinton-Trump-Corpus/Clinton/','Clinton_.*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump_raw = trump_corpus.raw()\n",
    "trump_words = trump_corpus.words()\n",
    "trump_text = nltk.Text(trump_words)\n",
    "\n",
    "clinton_raw = clinton_corpus.raw()\n",
    "clinton_words = clinton_corpus.words()\n",
    "clinton_text = nltk.Text(clinton_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the data\n",
    "Get rid of applause, stop words, anything between < >, punctuation \" -- . , '\n",
    "Write a for loop to clean up all the data \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this is a function that filters out the stopwords, punctuation, and audience directions in a given corpus\n",
    "\n",
    "def clean_up_data(x):\n",
    "    filtered_for_punctuation = x\n",
    "    filtered_for_punctuation = re.sub(\"[\\(\\<].*[\\)\\>]\", \"\", filtered_for_punctuation)\n",
    "    for punc in punctuation:\n",
    "        filtered_for_punctuation = filtered_for_punctuation.replace(punc, \"\") #remove punctuation\n",
    "    tokens = nltk.wordpunct_tokenize(filtered_for_punctuation) #tokenize text\n",
    "    #filtered_for_punctuation = [word for word in tokens if word.lower() not in stopwords.words('english')] #remove stopwords\n",
    "        #note that stopwords include words like very and against \n",
    "        #remove audience reactions like applause and laughter located between carrots and \n",
    "        #parentheses using regex\n",
    "    filtered_for_punctuation = str(filtered_for_punctuation)\n",
    "    return filtered_for_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Wdnum</th>\n",
       "      <th>ValMn</th>\n",
       "      <th>ValSD</th>\n",
       "      <th>AroMn</th>\n",
       "      <th>AroSD</th>\n",
       "      <th>DomMn</th>\n",
       "      <th>DomSD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abduction</td>\n",
       "      <td>621</td>\n",
       "      <td>2.76</td>\n",
       "      <td>2.06</td>\n",
       "      <td>5.53</td>\n",
       "      <td>2.43</td>\n",
       "      <td>3.49</td>\n",
       "      <td>2.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>able</td>\n",
       "      <td>1041</td>\n",
       "      <td>6.74</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2.17</td>\n",
       "      <td>6.83</td>\n",
       "      <td>2.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abortion</td>\n",
       "      <td>622</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.30</td>\n",
       "      <td>5.39</td>\n",
       "      <td>2.80</td>\n",
       "      <td>4.59</td>\n",
       "      <td>2.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>absent</td>\n",
       "      <td>1042</td>\n",
       "      <td>3.69</td>\n",
       "      <td>1.72</td>\n",
       "      <td>4.73</td>\n",
       "      <td>1.76</td>\n",
       "      <td>4.35</td>\n",
       "      <td>1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>absurd</td>\n",
       "      <td>623</td>\n",
       "      <td>4.26</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.36</td>\n",
       "      <td>2.20</td>\n",
       "      <td>4.73</td>\n",
       "      <td>1.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word  Wdnum  ValMn  ValSD  AroMn  AroSD  DomMn  DomSD\n",
       "0  abduction    621   2.76   2.06   5.53   2.43   3.49   2.38\n",
       "1       able   1041   6.74   2.00   4.30   2.17   6.83   2.04\n",
       "2   abortion    622   3.50   2.30   5.39   2.80   4.59   2.54\n",
       "3     absent   1042   3.69   1.72   4.73   1.76   4.35   1.87\n",
       "4     absurd    623   4.26   1.82   4.36   2.20   4.73   1.72"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load ANEW sentiment dictionary\n",
    "anew_df = pd.read_csv('https://github.com/peachypunk/NLTK_Final_Project/raw/master/ANEW2010_CSV.csv')\n",
    "anew_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               abduction  able  abortion  absent  absurd  abundance  abuse  \\\n",
      "clean_clinton        0.0  21.0       0.0     0.0     1.0        0.0    1.0   \n",
      "clean_trump          0.0  57.0       0.0     0.0     0.0        0.0    0.0   \n",
      "\n",
      "               accept  acceptance  access  ...   yellow  yelp  yolk  young  \\\n",
      "clean_clinton     2.0         1.0     2.0  ...      0.0   0.0   0.0    7.0   \n",
      "clean_trump       4.0         1.0     7.0  ...      1.0   0.0   0.0    7.0   \n",
      "\n",
      "               youth  zeal  zealous  zest  zipper  zoom  \n",
      "clean_clinton    0.0   0.0      0.0   0.0     0.0   0.0  \n",
      "clean_trump     10.0   0.0      0.0   0.0     0.0   0.0  \n",
      "\n",
      "[2 rows x 2476 columns]\n"
     ]
    }
   ],
   "source": [
    "#clean up both corpora using the custom function from above\n",
    "clean_trump = clean_up_data(trump_raw)\n",
    "clean_clinton = clean_up_data(clinton_raw)\n",
    "\n",
    "#join the corpora together while keeping them as separate entities as part of one \n",
    "#larger corpus umbrella\n",
    "collected_corpora_df = {'clean_clinton' : clean_clinton, 'clean_trump' : clean_trump}\n",
    "pd.Series(collected_corpora_df)\n",
    "\n",
    "#take the \"Word\" column from the anew_df and convert it into a list called \"wordlist\"\n",
    "wordlist = anew_df[\"Word\"].tolist()\n",
    "\n",
    "#initialize an empty array for the summed word counts for each w in wordlist\n",
    "#these will be summed across all corpora in the larger corpus as well\n",
    "wordfreq_corpus = [] \n",
    "\n",
    "#make an empty matrix that's the size of the ANEX words and two corpora \n",
    "matrix = np.zeros((len(collected_corpora_df), len(wordlist)))\n",
    "for i, cid in enumerate(collected_corpora_df): #for each corpus in the list of corpora\n",
    "    this_corpus_words = collected_corpora_df[(cid)]\n",
    "    for j, w in enumerate(wordlist): #for each word in the ANEW wordlist...\n",
    "        count = this_corpus_words.count(w) #count how many times each word (w) occurs in the wordlist for each corpus\n",
    "        matrix[i,j] = count\n",
    "        \n",
    "df = pd.DataFrame(matrix)\n",
    "df.columns = wordlist\n",
    "df.index = collected_corpora_df.keys()\n",
    "print(df)\n",
    "\n",
    "#optional: print the output to a csv file. just change the \"path_or_buf\" part to be where you\n",
    "#want to save the file\n",
    "#df.to_csv(path_or_buf='/Users/angelanazarian/nltk_output.csv', sep=',', header=True, index=True, line_terminator='\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
