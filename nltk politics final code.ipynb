{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our final project, Zoe, Nene and Angela used the NLTK package to perform sentiment analysis of a series of transcripts of speeches delivered by the 2016 presidential candidates, Hillary Clinton and Donald Trump. \n",
    "\n",
    "The ANEW dictionary was used (more details provided under its own subsection below) to generate sentiment values for each word in their dictionary. Values were provided for three domains: valence, arousal, and dominance. All three values are scored from 1 to 9 and the scores for each word were used as subsequent weights. \n",
    "\n",
    "Prior to performing our analysis, we hypothesized that: \n",
    "\n",
    "-Clinton would have a more positive average valence score across all her speeches compared to Trump\n",
    "\n",
    "-Trump would have a higher arousal score than Clinton\n",
    "\n",
    "-Trump would have a higher dominance score than Clinton\n",
    "\n",
    "The text used for our analysis can be found here: https://github.com/peachypunk/NLTK_Final_Project/tree/master/Clinton-Trump-Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Z/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import the packages\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "nltk.download(\"stopwords\") #Import stopwords and punctuation from NLTK\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and form corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump_corpus = nltk.corpus.PlaintextCorpusReader('Clinton-Trump-Corpus/Trump/','Trump_.*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clinton_corpus = nltk.corpus.PlaintextCorpusReader('Clinton-Trump-Corpus/Clinton/','Clinton_.*.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we are doing conversions between the \"raw\" text, the tokenized \"words\", and the plain \"text\" type. This will be used later because some functions require a certain object type, and some methods are only callable from certain object types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump_raw = trump_corpus.raw()\n",
    "trump_words = trump_corpus.words()\n",
    "trump_text = nltk.Text(trump_words)\n",
    "\n",
    "clinton_raw = clinton_corpus.raw()\n",
    "clinton_words = clinton_corpus.words()\n",
    "clinton_text = nltk.Text(clinton_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the data\n",
    "Get rid of applause, stop words, anything between < >, punctuation \" -- . , '\n",
    "Write a for loop to clean up all the data \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this is a function that filters out the stopwords, punctuation, and audience directions in a given corpus\n",
    "\n",
    "def clean_up_data(x):\n",
    "    filtered_for_punctuation = x\n",
    "    filtered_for_punctuation = filtered_for_punctuation.lower() #convert all words to lowercase\n",
    "    filtered_for_punctuation = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", filtered_for_punctuation) #remove parentheses\n",
    "    filtered_for_punctuation = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", filtered_for_punctuation) #remove carrots    for punc in punctuation:\n",
    "    for punc in punctuation:\n",
    "        filtered_for_punctuation = filtered_for_punctuation.replace(punc, \"\") #remove punctuation\n",
    "    filtered_for_punctuation = nltk.wordpunct_tokenize(filtered_for_punctuation) #tokenize text\n",
    "    filtered_for_punctuation = [word for word in filtered_for_punctuation if word.lower() not in stopwords.words('english')] #remove stopwords\n",
    "        #note that stopwords include words like very and against \n",
    "    return filtered_for_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clean up both corpora using the custom function from above\n",
    "clean_trump = clean_up_data(trump_raw)\n",
    "clean_clinton = clean_up_data(clinton_raw)\n",
    "\n",
    "#join the corpora together while keeping them as separate entities as part of one \n",
    "#larger corpus umbrella\n",
    "collected_corpora_df = {'clean_clinton' : clean_clinton, 'clean_trump' : clean_trump}\n",
    "pd.Series(collected_corpora_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the \"ANEW\" sentiment dictionary\n",
    "\"ANEW\" stands for \"Affective Norms for English Words\".\n",
    "It is a list of 2476 words that have been normed for ratings on 3 affective dimensions: Valence, Arousal, and Dominance. The rating is on a Likert Scale ranging from 1 to 9, with a higher rating indicating higher valence, arousal, or dominance, respectively. Each word has a mean valence, arousal, and dominance rating, as well as the associated standard deviations for each mean.\n",
    "\"Wdnum\" is presumably the \"word number\" or an arbitrary number label assigned to each word. We won't be using this variable in our sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load ANEW sentiment dictionary\n",
    "anew_df = pd.read_csv('https://github.com/peachypunk/NLTK_Final_Project/raw/master/ANEW2010_CSV.csv')\n",
    "anew_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we're taking all the words in anew_df and storing it in a list called \"wordlist\". This will make it easier to do word counting later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#take the \"Word\" column from the anew_df and convert it into a list called \"wordlist\"\n",
    "wordlist = anew_df[\"Word\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform word counting\n",
    "To perform sentiment analysis based on the words from ANEW, we will first perform a word count of how many times each ANEW word appears in the Trump corpus and Clinton corpus, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialize an empty array for the summed word counts for each w in wordlist\n",
    "#these will be summed across all corpora in the larger corpus as well\n",
    "wordfreq_corpus = [] \n",
    "\n",
    "#make an empty matrix that's the size of the ANEX words and two corpora \n",
    "matrix = np.zeros((len(collected_corpora_df), len(wordlist)))\n",
    "for i, cid in enumerate(collected_corpora_df): #for each corpus in the list of corpora\n",
    "    this_corpus_words = collected_corpora_df[(cid)]\n",
    "    for j, w in enumerate(wordlist): #for each word in the ANEW wordlist...\n",
    "        count = this_corpus_words.count(w) #count how many times each word (w) occurs in the wordlist for each corpus\n",
    "        matrix[i,j] = count\n",
    "        \n",
    "df = pd.DataFrame(matrix)\n",
    "df.columns = wordlist\n",
    "df.index = collected_corpora_df.keys()\n",
    "#print(df)\n",
    "\n",
    "#optional: print the output to a csv file. just change the \"path_or_buf\" part to be where you\n",
    "#want to save the file\n",
    "#df.to_csv(path_or_buf='/Users/angelanazarian/nltk_output.csv', sep=',', header=True, index=True, line_terminator='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We successfully performed word counting: the above output contains the count for each ANEW word in the Clinton corpus and Trump corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform calculation of weighted MEANS for valence, arousal, and dominance (for each word)\n",
    "Using the word counts we obtained above, we will now calculate each weighted means separately for each affective dimension (valenc, arousal, dominance) and for each corpus (Trump, Clinton).\n",
    "\n",
    "For each word \"w\" in the ANEW word list: Weighted mean = sum of [count of \"w\" in the corpus x mean affective rating of \"w\" from ANEW], for each corpus\n",
    "\n",
    "Below, we are converting the word count dataframe \"df\" into a transposed dataframe with sensible row indices and column labels. Essentially, we're converting \"df\" from wide format into long format to make it easier to calculate the weighted means. The resulting dataframe is called \"df_long\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_wide = df.copy() #make a copy of df and calling it \"df_wide\"\n",
    "df_long = df_wide.transpose() #transpose \"df_wide\" into \"df_long\" format\n",
    "df_long.reset_index(level=0, inplace=True) #converting the ANEW word indices into numeric indices\n",
    "df_long.columns = ['Word', 'clinton_WC', 'trump_WC'] #renaming columns (WC = word count)\n",
    "df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "anew_sliced = anew_df[['Word','ValMn', 'AroMn', 'DomMn']] \n",
    "anew_sliced #subsetting the mean ratings from ANEW df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_combined = pd.merge(df_long, anew_sliced)\n",
    "df_combined #combining df_long with anew_sliced into one dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform the calculation of weighted means for each word.\n",
    "\n",
    "Below, each line of code calculates the product of each word's count in the corpus with the associated mean ratings of Valence, Arousal, and Dominance, and stores the value in a new appended column. The column multiplications are calcualted for all the rows (words) in the dataframe.\n",
    "\n",
    "For example, the first line of code (below) calculates the weighted valence of each word in the Clinton corpus, based on how frequently that word occurs in the corpus (the word count). It takes clinton_WC and multiplies it by ValMn, and stores it in a new column called \"clinton_Val\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# WORD COUNTS x VALENCE MEANS:\n",
    "\n",
    "# \"clinton_Val\" = clinton_WC x ValMn\n",
    "df_combined['clinton_Val'] = df_combined.apply(lambda row: (row['clinton_WC']*row['ValMn']), axis=1)\n",
    "\n",
    "# \"trump_Val\" = trump_WC x ValMn\n",
    "df_combined['trump_Val'] = df_combined.apply(lambda row: (row['trump_WC']*row['ValMn']), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# WORD COUNTS x AROUSAL MEANS:\n",
    "\n",
    "# \"clinton_Aro\" = clinton_WC x AroMn\n",
    "df_combined['clinton_Aro'] = df_combined.apply(lambda row: (row['clinton_WC']*row['AroMn']), axis=1)\n",
    "\n",
    "# \"trump_Aro\" = trump_WC x AroMn\n",
    "df_combined['trump_Aro'] = df_combined.apply(lambda row: (row['trump_WC']*row['AroMn']), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# WORD COUNTS x DOMINANCE MEANS:\n",
    "\n",
    "# \"clinton_Dom\" = clinton_WC x DomMn\n",
    "df_combined['clinton_Dom'] = df_combined.apply(lambda row: (row['clinton_WC']*row['DomMn']), axis=1)\n",
    "\n",
    "# \"trump_Val\" = trump_WC x ValMn\n",
    "df_combined['trump_Dom'] = df_combined.apply(lambda row: (row['trump_WC']*row['DomMn']), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform calculation of weighted MEANS for valence, arousal, and dominance (across all words)\n",
    "\n",
    "Now that we have the weighted means *for each word*, we can calculate the weighted means across all words (for Trump and Clinton, respectively). This would be the overall \"affectiveness\" of each corpus on each affective dimension, based on the words in ANEW. It is calculated by summing the weighed means across all the words, and dividing that sum by the total number of words from that category (valence, arousal, or dominance) that were used by that candidate. This provided us with an average measure for each of those values. This approach was necessary because ANEW uses a 1-9 likert scale to qualify each of the measures rather than using negative numbers. So merely summing the values doesn’t reflect whether each candidate was more negative or positive. \n",
    "\n",
    "\n",
    "#### Overall weighted means for VALENCE in each corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Mean of all values in \"clinton_Val\" column (summing across all words)\n",
    "Mean_clinton_Val = (df_combined['clinton_Val'].values.sum())/((df_combined['clinton_Val'] != 0).sum())\n",
    "print(Mean_clinton_Val)\n",
    "\n",
    "#Sum of all values in \"trump_Val\" column (summing across all words)\n",
    "Mean_trump_Val = (df_combined['trump_Val'].values.sum())/((df_combined['trump_Val'] != 0).sum())\n",
    "print(Mean_trump_Val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted means for AROUSAL in each corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sum of all values in \"clinton_Aro\" column (summing across all words)\n",
    "Mean_clinton_Aro = (df_combined['clinton_Aro'].values.sum())/((df_combined['clinton_Aro'] != 0).sum())\n",
    "print(Mean_clinton_Aro)\n",
    "\n",
    "#Sum of all values in \"trump_Aro\" column (summing across all words)\n",
    "Mean_trump_Aro = (df_combined['trump_Aro'].values.sum())/((df_combined['trump_Aro'] != 0).sum()) \n",
    "print(Mean_trump_Aro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted means for DOMINANCE in each corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sum of all values in \"clinton_Dom\" column (summing across all words)\n",
    "Mean_clinton_Dom = (df_combined['clinton_Dom'].values.sum())/((df_combined['clinton_Dom'] != 0).sum())\n",
    "print(Mean_clinton_Dom)\n",
    "\n",
    "#Sum of all values in \"trump_Dom\" column (summing across all words)\n",
    "Mean_trump_Dom = (df_combined['trump_Dom'].values.sum())/((df_combined['trump_Dom'] != 0).sum())\n",
    "print(Mean_trump_Dom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 6 weighted means that we just calculated for each corpus, we will store them in a new dataframe so it's easier to compare the values.\n",
    "\n",
    "First, we'll store the weighted means in 2 separate lists for Trump and Clinton, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Storing the calculated weighted means in 2 separate lists for Trump corpus and Clinton corpus\n",
    "trump_summed = ['Trump', Mean_trump_Val, Mean_trump_Aro, Mean_trump_Dom]\n",
    "clinton_summed = ['Clinton', Mean_clinton_Val, Mean_clinton_Aro, Mean_clinton_Dom]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will combine the two lists into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creating a new dataframe that will store the calculated weighted means\n",
    "sums_df = pd.DataFrame({'Trump': trump_summed,'Clinton': clinton_summed})\n",
    "sums_df = sums_df.transpose()\n",
    "sums_df.columns = ['Candidate', 'Valence', 'Arousal', 'Dominance']\n",
    "sums_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#range of valence for each candidate \n",
    "clinton_max = df_combined[['clinton_Val']].max(axis=0)/((df_combined['clinton_Val'] != 0).sum())\n",
    "trump_max = df_combined[['trump_Val']].max(axis=0)/((df_combined['trump_Val'] != 0).sum())\n",
    "\n",
    "clinton_min = df_combined[['clinton_Val']].min(axis=0)/((df_combined['clinton_Val'] != 0).sum())\n",
    "trump_min = df_combined[['trump_Val']].min(axis=0)/((df_combined['trump_Val'] != 0).sum())\n",
    "\n",
    "print('Clinton', clinton_min, clinton_max)\n",
    "print('Trump', trump_min, trump_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical analysis: Performing t-test on the weighted means for each corpus ##\n",
    "\n",
    "Next, we decided to perform a t-test to see whether the weighted means for valence, arousal, and dominance for Clinton and Trump were statistically significant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independent samples t-test: comparing the weighted mean **VALENCE** ratings for Clinton vs. Trump:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a and b as the weighted valence means for Clinton and Trump (pulled out the relevant columns from df_combined)\n",
    "a = df_combined['clinton_Val'].values\n",
    "b = df_combined['trump_Val'].values\n",
    "\n",
    "\n",
    "# perform t-test\n",
    "t, p = ttest_ind(a, b, equal_var=False)\n",
    "print(\"ttest_ind:            t = %g  p = %g\" % (t, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independent samples t-test: comparing the weighted mean **AROUSAL** ratings for Clinton vs. Trump:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a and b as the weighted arousal means for Clinton and Trump (pulled out the relevant columns from df_combined)\n",
    "a = df_combined['clinton_Aro'].values\n",
    "b = df_combined['trump_Aro'].values\n",
    "\n",
    "\n",
    "# perform t-test\n",
    "t, p = ttest_ind(a, b, equal_var=False)\n",
    "print(\"ttest_ind:            t = %g  p = %g\" % (t, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independent samples t-test: comparing the weighted mean **DOMINANCE** ratings for Clinton vs. Trump:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a and b as the weighted dominance means for Clinton and Trump (pulled out the relevant columns from df_combined)\n",
    "a = df_combined['clinton_Dom'].values\n",
    "b = df_combined['trump_Dom'].values\n",
    "\n",
    "\n",
    "# perform t-test\n",
    "t, p = ttest_ind(a, b, equal_var=False)\n",
    "print(\"ttest_ind:            t = %g  p = %g\" % (t, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above confirms that the overall weighted mean ratings for valence, arousal, and dominance in the Trump corpus vs. Clinton corpus were statistically significant. Trump had significantly higher weighed means on all 3 dimensions of affectiveness, suggesting that Trump's speeches had higher valence (t = -5.34, p <.001), arousal (t = -5.82, p < .0001), and dominance (t = -5.55, p < .001) than Clinton's speeches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of the calculated weighted means:\n",
    "\n",
    "This first round of analysis demonstrates that by comparing average values of valence, arousal, and dominance across all campaign speeches for both candidates, Trump emerged as the front runner on all three accounts. \n",
    "\n",
    "His speeches were higher in valence, the content had a higher arousal rate, and he displayed greater levels of dominance in his speech compared to Hillary. The valence findings initially threw me off, and were a bit unexpected...until we realized how many positive qualifiers and adjectives he tended to use in his speeches (i.e. huuuuuuge, very, incredible, best). In a way, this made me realize how it could be easy for his fans and listeners to be swayed by this exaggerated positive language, and gloss over the bullying, empty quality of his speeches. \n",
    "\n",
    "Trump's higher valence ratings also match this story of greater exaggeration use. When examining the range of valence exhibited by each candidate, Trump had a larger emotional valence range than Hillary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "### Most frequent words for each candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Calculate frequency distribution\n",
    "clinton_fdist = nltk.FreqDist(clean_clinton)\n",
    "top20clinton = pd.DataFrame(clinton_fdist.most_common(20))\n",
    "top20clinton.columns = ['Word', 'frequency']\n",
    "\n",
    "trump_fdist = nltk.FreqDist(clean_trump)\n",
    "trump_fdist.most_common(20)\n",
    "top20trump = pd.DataFrame(trump_fdist.most_common(20))\n",
    "top20trump.columns = ['Word', 'frequency']\n",
    "print(top20clinton)\n",
    "print(top20trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#top 20 words for both candidates in one dataframe\n",
    "top20words = pd.merge(top20trump, top20clinton, on='Word', how='outer')\n",
    "top20words.columns = ['Word', 'frequency_trump', 'frequency_clinton']\n",
    "top20words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot top words for both candidates to see how much each person used those words\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Initialize the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "\n",
    "# Plot the top word frequency for Trump\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(x=\"Word\", y=\"frequency_trump\", data=top20words,\n",
    "            label=\"Trump\", color=\"b\")\n",
    "\n",
    "#Plot the top word frequency for Clinton\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x=\"Word\", y=\"frequency_clinton\", data=top20words,\n",
    "            label=\"Clinton\", color=\"b\")\n",
    "\n",
    "# Add a legend and informative axis label\n",
    "ax.legend(ncol=2, loc=\"upper right\", frameon=True)\n",
    "ax.set(xlim=(0, 30), ylabel=\"\",\n",
    "       xlabel=\"Frequency of top words for each presidential candidate in 2016\")\n",
    "sns.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top words used by each candidate (along with visuals for valence of those words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#most frequently used words for each candidate along with valence\n",
    "#make a new dataframe that includes valence values along with top20words \n",
    "top20valence = pd.merge(anew_sliced, top20words, on='Word', how='right')\n",
    "top20valence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: many of the top 20 words for each candidate don't appear in ANEW. This issue is addressed in another section of our analyses below aptly titled \"Words NOT Captured by ANEW\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a graph of the most frequently used words for each candidate, and how many times each of them used it. You can hover over the nodes to display the actual words.\n",
    "\n",
    "Something that stood out to me as interesting was which words were exclusive to each of the candidates (appear along the axes). For instance, although Trump was all about boosting employment and reinvigorating the coal mining industry, it was actually Hillary who used the word \"work\" the most. Trump's unique most frequently used word was \"right\"--which also seemingly speaks volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "plotly.tools.set_credentials_file(username='peachypunk', api_key='zl14cGCYwyddRaQklh90')\n",
    "\n",
    "\n",
    "data = [\n",
    "    go.Scatter(\n",
    "        x= top20valence.frequency_trump,\n",
    "        y=top20valence.frequency_clinton,\n",
    "        mode='markers',\n",
    "        text=top20valence.Word\n",
    "    )\n",
    "]\n",
    "layout = go.Layout(\n",
    "    title='Words Most Frequently Used By Each Candidate',\n",
    "    xaxis = dict(\n",
    "        title='Trump Frequency'),\n",
    "    yaxis = dict(\n",
    "        title= 'Clinton Frequency'))\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an even cooler version of the graph from above. This visualization incorporates the ratings or sentiment scores for each word by coloring the nodes according to their valence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    go.Scatter(\n",
    "        x= top20valence.frequency_trump,\n",
    "        y=top20valence.frequency_clinton,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "        size='16',\n",
    "        color = top20valence.ValMn, #set color equal to a variable\n",
    "        colorscale='YlOrRd',\n",
    "        showscale=True\n",
    "    ),\n",
    "        text=top20valence.Word\n",
    "    )\n",
    "]\n",
    "layout = go.Layout(\n",
    "    title='Words Most Frequently Used By Each Candidate',\n",
    "    xaxis = dict(\n",
    "        title='Trump Frequency'),\n",
    "    yaxis = dict(\n",
    "        title= 'Clinton Frequency'))\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something else we were interested in examining is the common text analysis measure of lexical diversity. Lexical richness or lexical diversity reflects the number of distinct words used by each candidate: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Clinton', len(set(clean_clinton)) / len(clean_clinton))\n",
    "print('Trump', len(set(clean_trump)) / len(clean_trump))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Clinton's speeches contained a more diverse set of words overall compared to Trump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'We're Hip and With It': Or How I Learned to Target the Youth\n",
    "\n",
    "Something else we were interested in examining was the prevalence of social media and related terms used by each candidate. We anticipated that incorporating this language served two purposes: it allowed the candidates to seem up to date with tech and culture, and more importantly it allowed them to reach out to the base of young voters that both Trump and Clinton were interested in recruiting. \n",
    "\n",
    "We expected that Hillary was aiming to recruit the youth more actively. This was particularly reflected by the use of memes in her campaign, her appearances on shows popular with twenty-somethings (i.e. Broad City), and maintaining a carefully pruned image on various social media platforms. \n",
    "\n",
    "However, it's also important to consider that Trump is a yuuuuuge Tweeter, and he was heavily embroiled in the blame game re:Hillary's emails at the time. So if he seemed to use more social media terms, I wouldn't exactly be surprised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#count how often a word occurs in a text, and compute what percentage of the text is taken \n",
    "#up by a specific word. \n",
    "#we can do this for social media words (i.e. Facebook, tweet, Twitter, social media, online)\n",
    "\n",
    "from collections import Counter\n",
    "social_media_counts_clinton = Counter()\n",
    "words = ('facebook', 'tweet', 'twitter', 'social media', 'online', 'email', 'emails', 'blog', 'google', 'hashtag', 'selfie', 'viral')\n",
    "for word in clean_clinton:\n",
    "    if word in words:\n",
    "        social_media_counts_clinton[word] += 1\n",
    "print (social_media_counts_clinton)\n",
    "\n",
    "#compute the percentage of their speech text that consists of words related to social media \n",
    "social_media_counts_clinton = pd.DataFrame(social_media_counts_clinton, index=['count'])\n",
    "social_media_sum_clinton = social_media_counts_clinton.sum(axis =1)\n",
    "percent_soc_media_clinton = 100 * (social_media_sum_clinton / len(clean_clinton))\n",
    "percent_soc_media_clinton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "social_media_counts_trump = Counter()\n",
    "words = ('facebook', 'tweet', 'twitter', 'social media', 'online', 'email', 'emails', 'blog', 'google', 'hashtag', 'selfie', 'viral')\n",
    "for word in clean_trump:\n",
    "    if word in words:\n",
    "        social_media_counts_trump[word] += 1\n",
    "print (social_media_counts_trump)\n",
    "\n",
    "#compute the percentage of their speech text that consists of words related to social media \n",
    "social_media_counts_trump = pd.DataFrame(social_media_counts_trump, index=['count'])\n",
    "social_media_sum_trump = social_media_counts_trump.sum(axis =1)\n",
    "percent_soc_media_trump = 100 * (social_media_sum_trump / len(clean_trump))\n",
    "percent_soc_media_trump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that .32% of Trump's speeches consisted of words related to social media/tech, whereas Clinton's speeches consisted of only .13%. \n",
    "\n",
    "This difference between candidates seems to provide further evidence of the role of the email scandal and how predominantly it was featured as a rhetorical tactic in his speeches. Clinton may have intentionally avoided using certain terms (i.e. \"email\") in order to not bring further attention to a topic that was bringing her negative press at the time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Words NOT captured by ANEW\n",
    "After getting the weighted sums, we were curious about the words in the two corpora that were NOT in the ANEW list. Perhaps those words can give us better insight into the \"affectiveness\" of each presidential candidate's speeches.\n",
    "\n",
    "To explore that question, we'll filter out the ANEW words from each corpus and examine the remaining words.\n",
    "\n",
    "This is a function that filters out the ANEW words from the corpora.\n",
    "The function will prune a text input by excluding a set of words. It takes \"text_input\" as the input. In the input, it'll exclude words that are in \"words_to_exclude\". It returns a list of words called \"pruned_list\", which contains the leftover words that WEREN'T in the \"words_to_exclude\" list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def excludewords(text_input, words_to_exclude):\n",
    "    pruned_list = []\n",
    "    for w in text_input:\n",
    "        if w not in words_to_exclude:\n",
    "            pruned_list.append(w)\n",
    "    return pruned_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll apply excludewords() on the cleaned trump corpus and clinton corpus, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump_pruned = excludewords(clean_trump, wordlist)\n",
    "clinton_pruned = excludewords(clean_clinton, wordlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many words weren't captured by ANEW:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(trump_pruned))\n",
    "print(len(clinton_pruned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are 37453 words in Trump corpus and 11670 words in Clinton corpus that were NOT in the ANEW list. Many of these words seem to have high valence, so we planned to do additional sentiment analyses on these words. However, we didn't have enough time to perform this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Considerations and Next Steps\n",
    "\n",
    "Some other ideas we talked about implementing for this project but didn't get time to include are: \n",
    "\n",
    "-a \"how presidential\" function. This function would run through the corpus of all presidential inaugural speeches to establish which terms are common across all the presidents to develop an index of \"presidential speech\", so to speak (pun not intended). \n",
    "\n",
    "This corpus of presidential speech would then be used as a comparative point for the speeches delivered by both presidential candidate. In other words, how many of those presidential words did Clinton and Trump employ in their own speeches? This function could be fun to use in other ways too, including speeches delivered by fake presidents on TV shows and in films. \n",
    "\n",
    "We could also pull popularity ratings for each president over time and include that determine what emotional valence qualities were associated with the most popular presidents. For example, JFK was very well liked. Was he more emotional in his speeches? Was Clinton? \n",
    "\n",
    "We could then plot each candidate’s location on the presidential language curve in a graph as well. \n",
    "\n",
    "-we could also consider coming up with our own list of custom stopwords to exclude since some of the ones included in the NLTK set seemed to actually be relevant to our analysis. \n",
    "\n",
    "-something we tried to achieve, but ran out of time to work on was reclassifying the words that were not captured by ANEW\n",
    "    We could either reclassify the words ourselves manually, or use unsupervised machine learning to first group them and then apply our own valence grades\n",
    "    \n",
    "-find words similar to the tagged words in ANEW as a way to expand the ANEW corpus and getting word counts for those “similar words” too (perhaps using Wikipedia lists that Drew linked us to or Wordnet)\n",
    "\n",
    "-we were also interested in doing a separate analysis of exaggeration words, positive qualifiers, and adjectives\n",
    "\n",
    "-visualize the text as a network by making a scatterplot that shows co-occurring words using concordance\n",
    "\n",
    "-find entire sets of words that only occurred once in each corpus (aka hapaxes)\n",
    "\n",
    "-decipher important measures to each person (i.e. social media presence, china, wall, obamacare, etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Print Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The printtext() function is a simple function that outputs a corpus or tokenizer variable to a textfile in the current filepath. It's sometimes handy if you'd like to see/check what the cleaned corpus looks like after you run it through the text cleaning.\n",
    "\n",
    "It takes two arguments- the name of the corpus variable to export, and the title of the file that you would like to enter in a string format- 'example_name'.\n",
    "\n",
    "example:\n",
    "printtext(text to export, 'name of txt file')\n",
    "exporttext(clean_trump, 'testexport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def exporttext(text2print, file_title):\n",
    "    \n",
    "    #export clean_trump to text file\n",
    "    file_t = file_title\n",
    "    trump_print = ' '.join(text2print)\n",
    "    file = open(file_t + '.txt','w') \n",
    "    file.write(str(trump_print)) \n",
    "    file.close() \n",
    "    return \"Text exported to enclosing directory!\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
