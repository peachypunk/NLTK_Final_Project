{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angelanazarian/Applications/miniconda3/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/angelanazarian/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import the packages\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "nltk.download(\"stopwords\") #Import stopwords and punctuation from NLTK\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump_corpus = nltk.corpus.PlaintextCorpusReader('/Users/angelanazarian/NLTK_Final_Project/Clinton-Trump-Corpus/Trump/','Trump_.*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clinton_corpus = nltk.corpus.PlaintextCorpusReader('/Users/angelanazarian/NLTK_Final_Project/Clinton-Trump-Corpus/Clinton/','Clinton_.*.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the data\n",
    "Get rid of applause, stop words, anything between < >, punctuation \" -- . , '\n",
    "Write a for loop to clean up all the data \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trump_raw = trump_corpus.raw()\n",
    "# trump_words = trump_corpus.words()\n",
    "# trump_text = nltk.Text(trump_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump_test = nltk.corpus.PlaintextCorpusReader('/Users/angelanazarian/NLTK_Final_Project/Clinton-Trump-Corpus/test_trump/','Trump_.*.txt')\n",
    "trump_raw = trump_test.raw()\n",
    "trump_words = trump_test.words()\n",
    "trump_text = nltk.Text(trump_words)\n",
    "\n",
    "clinton_test = nltk.corpus.PlaintextCorpusReader('/Users/angelanazarian/NLTK_Final_Project/Clinton-Trump-Corpus/test_clinton/','Clinton_.*.txt')\n",
    "clinton_raw = clinton_test.raw()\n",
    "clinton_words = clinton_test.words()\n",
    "clinton_text = nltk.Text(clinton_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write a function here\n",
    "#filter out the stopwords, punctuation, and audience directions in the text \n",
    "\n",
    "def clean_up_data(x):\n",
    "    filtered_for_punctuation = x\n",
    "    filtered_for_punctuation = re.sub(\"[\\(\\<].*[\\)\\>]\", \"\", filtered_for_punctuation)\n",
    "    for punc in punctuation:\n",
    "        filtered_for_punctuation = filtered_for_punctuation.replace(punc, \"\") #remove punctuation\n",
    "    tokens = nltk.wordpunct_tokenize(filtered_for_punctuation) #tokenize text\n",
    "    filtered_for_punctuation = [word for word in tokens if word.lower() not in stopwords.words('english')] #remove stopwords\n",
    "        #note that stopwords include words like very and against \n",
    "        #remove audience reactions like applause and laughter located between carrots and \n",
    "        #parentheses using regex\n",
    "    filtered_for_punctuation = str(filtered_for_punctuation)\n",
    "    return filtered_for_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clean_trump', 'clean_clinton']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean_up_data(clinton_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Wdnum</th>\n",
       "      <th>ValMn</th>\n",
       "      <th>ValSD</th>\n",
       "      <th>AroMn</th>\n",
       "      <th>AroSD</th>\n",
       "      <th>DomMn</th>\n",
       "      <th>DomSD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abduction</td>\n",
       "      <td>621</td>\n",
       "      <td>2.76</td>\n",
       "      <td>2.06</td>\n",
       "      <td>5.53</td>\n",
       "      <td>2.43</td>\n",
       "      <td>3.49</td>\n",
       "      <td>2.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>able</td>\n",
       "      <td>1041</td>\n",
       "      <td>6.74</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2.17</td>\n",
       "      <td>6.83</td>\n",
       "      <td>2.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abortion</td>\n",
       "      <td>622</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.30</td>\n",
       "      <td>5.39</td>\n",
       "      <td>2.80</td>\n",
       "      <td>4.59</td>\n",
       "      <td>2.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>absent</td>\n",
       "      <td>1042</td>\n",
       "      <td>3.69</td>\n",
       "      <td>1.72</td>\n",
       "      <td>4.73</td>\n",
       "      <td>1.76</td>\n",
       "      <td>4.35</td>\n",
       "      <td>1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>absurd</td>\n",
       "      <td>623</td>\n",
       "      <td>4.26</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.36</td>\n",
       "      <td>2.20</td>\n",
       "      <td>4.73</td>\n",
       "      <td>1.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word  Wdnum  ValMn  ValSD  AroMn  AroSD  DomMn  DomSD\n",
       "0  abduction    621   2.76   2.06   5.53   2.43   3.49   2.38\n",
       "1       able   1041   6.74   2.00   4.30   2.17   6.83   2.04\n",
       "2   abortion    622   3.50   2.30   5.39   2.80   4.59   2.54\n",
       "3     absent   1042   3.69   1.72   4.73   1.76   4.35   1.87\n",
       "4     absurd    623   4.26   1.82   4.36   2.20   4.73   1.72"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inserting nene's code here \n",
    "import pandas as pd\n",
    "#load anew\n",
    "anew_df = pd.read_csv('https://github.com/peachypunk/NLTK_Final_Project/raw/master/ANEW2010_CSV.csv')\n",
    "anew_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   able  abortion  absent  absurd  abundance  abuse  accept  acceptance  \\\n",
      "0   0.0       0.0     0.0     0.0        0.0    0.0     0.0         0.0   \n",
      "1   0.0       0.0     0.0     0.0        0.0    0.0     0.0         0.0   \n",
      "\n",
      "   access  \n",
      "0     0.0  \n",
      "1     0.0  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "clean_trump = clean_up_data(trump_raw)\n",
    "clean_clinton = clean_up_data(clinton_raw)\n",
    "#collected_corpora = [clean_trump]+[clean_clinton] #it's only indexable by numbers though i.e. collected_corpora[0]\n",
    "collected_corpora_df = {'clean_clinton' : clean_clinton, 'clean_trump' : clean_trump}\n",
    "pd.Series(collected_corpora_df)\n",
    "\n",
    "\n",
    "#take the \"Word\" column from the anew_df and converting it into a list called \"wordlist\"\n",
    "wordlist = anew_df[\"Word\"].tolist()\n",
    "wordlist = wordlist[1:10]\n",
    "\n",
    "wordfreq_corpus = [] #the final output list of word frequencies (summed count)\n",
    "wordcounts_corpus = [] #the interim list for storing the count for each word (in each corpus)\n",
    "\n",
    "matrix = np.zeros((len(collected_corpora_df), len(wordlist)))\n",
    "for i, cid in enumerate(collected_corpora_df): #for each corpus in the list of corpora\n",
    "    this_corpus_words = collected_corpora_df[(cid)]\n",
    "    for j, w in enumerate(wordlist): #for each word in the ANEW wordlist...\n",
    "        #wordcounts_corpus = [0 + count]\n",
    "        count = this_corpus_words.count(w) #count how many times each word (w) occurs in the wordlist,\n",
    "        matrix[i,j] = count\n",
    "        #words_in_file.append(count)\n",
    "    #wordcounts_corpus.append(np.sum(words_in_file)) #append the count (for each w) to the \"wordcounts_trump\" list  \n",
    "    #wordfreq_corpus.append(sum(wordcounts_corpus)) #and store this in a list called \"wordfreq\" (note the indentation! outside of the fid loop, but inside the wordlist loop.)\n",
    "\n",
    "#print (matrix)\n",
    "df = pd.DataFrame(matrix)\n",
    "df.columns = wordlist\n",
    "#df.index = collected_corpora_df()\n",
    "print(df)\n",
    "#count = the count for each w in wordlist, in each fid in trump corpus\n",
    "#wordfreq = the summed word count for each w in wordlist, summed across all corpora in the larger corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
